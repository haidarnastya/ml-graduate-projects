{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Haidar_Anastasia_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Anastasia Haidar\n",
    "<br>\n",
    "Github Username: haidarnastya\n",
    "<br>\n",
    "USC ID: 1163-9833-46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: lying/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset8.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset9.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset10.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset11.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset12.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset13.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset14.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: lying/dataset15.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset8.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset9.csv\n",
      " Shape: (479, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset8.csv\n",
      " Shape: (479, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset9.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset8.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset9.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending1/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: bending2/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset10.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset11.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset12.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset13.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset14.csv\n",
      " Shape: (479, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: cycling/dataset15.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset10.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset11.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset12.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset13.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset14.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: sitting/dataset15.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset1.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset2.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset3.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset4.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset5.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset6.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset7.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset8.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset9.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset10.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset11.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset12.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset13.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset14.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: walking/dataset15.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset10.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset11.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset12.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset13.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset14.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Reading: standing/dataset15.csv\n",
      " Shape: (480, 7)\n",
      " Columns: ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
      "\n",
      "Total CSVs found: 88\n",
      "Available files: ['lying/dataset1.csv', 'lying/dataset2.csv', 'lying/dataset3.csv', 'lying/dataset4.csv', 'lying/dataset5.csv', 'lying/dataset6.csv', 'lying/dataset7.csv', 'lying/dataset8.csv', 'lying/dataset9.csv', 'lying/dataset10.csv', 'lying/dataset11.csv', 'lying/dataset12.csv', 'lying/dataset13.csv', 'lying/dataset14.csv', 'lying/dataset15.csv', 'cycling/dataset1.csv', 'cycling/dataset2.csv', 'cycling/dataset3.csv', 'cycling/dataset4.csv', 'cycling/dataset5.csv', 'cycling/dataset6.csv', 'cycling/dataset7.csv', 'cycling/dataset8.csv', 'cycling/dataset9.csv', 'sitting/dataset1.csv', 'sitting/dataset2.csv', 'sitting/dataset3.csv', 'sitting/dataset4.csv', 'sitting/dataset5.csv', 'sitting/dataset6.csv', 'sitting/dataset7.csv', 'sitting/dataset8.csv', 'sitting/dataset9.csv', 'walking/dataset1.csv', 'walking/dataset2.csv', 'walking/dataset3.csv', 'walking/dataset4.csv', 'walking/dataset5.csv', 'walking/dataset6.csv', 'walking/dataset7.csv', 'walking/dataset8.csv', 'walking/dataset9.csv', 'bending1/dataset1.csv', 'bending1/dataset2.csv', 'bending1/dataset3.csv', 'bending1/dataset4.csv', 'bending1/dataset5.csv', 'bending1/dataset6.csv', 'bending1/dataset7.csv', 'bending2/dataset1.csv', 'bending2/dataset2.csv', 'bending2/dataset3.csv', 'bending2/dataset4.csv', 'bending2/dataset5.csv', 'bending2/dataset6.csv', 'cycling/dataset10.csv', 'cycling/dataset11.csv', 'cycling/dataset12.csv', 'cycling/dataset13.csv', 'cycling/dataset14.csv', 'cycling/dataset15.csv', 'sitting/dataset10.csv', 'sitting/dataset11.csv', 'sitting/dataset12.csv', 'sitting/dataset13.csv', 'sitting/dataset14.csv', 'sitting/dataset15.csv', 'standing/dataset1.csv', 'standing/dataset2.csv', 'standing/dataset3.csv', 'standing/dataset4.csv', 'standing/dataset5.csv', 'standing/dataset6.csv', 'standing/dataset7.csv', 'standing/dataset8.csv', 'standing/dataset9.csv', 'walking/dataset10.csv', 'walking/dataset11.csv', 'walking/dataset12.csv', 'walking/dataset13.csv', 'walking/dataset14.csv', 'walking/dataset15.csv', 'standing/dataset10.csv', 'standing/dataset11.csv', 'standing/dataset12.csv', 'standing/dataset13.csv', 'standing/dataset14.csv', 'standing/dataset15.csv']\n"
     ]
    }
   ],
   "source": [
    "zip_path = './data/AReM.zip'\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "#open the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    #loop through all files in zip subfolders\n",
    "    for file_name in zip_ref.namelist():\n",
    "        if file_name.endswith('.csv'):\n",
    "            print(f\"Reading: {file_name}\")\n",
    "            \n",
    "            #read the csv file\n",
    "            with zip_ref.open(file_name) as file:\n",
    "                try:\n",
    "                    #define col names\n",
    "                    columns_names = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "                    #read csv with col names\n",
    "                    df = pd.read_csv(io.BytesIO(file.read()), comment='#', header=None, names=columns_names, on_bad_lines='skip')\n",
    "                    \n",
    "                    #store in dataframe dictionary\n",
    "                    dataframes[file_name] = df\n",
    "\n",
    "                    print(f\" Shape: {df.shape}\")\n",
    "                    print(f\" Columns: {list(df.columns)}\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\" Error reading {file_name}: {e}\\n\")\n",
    "                    continue\n",
    "\n",
    "print(f\"Total CSVs found: {len(dataframes)}\")\n",
    "print(f\"Available files: {list(dataframes.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape: (9120, 7)\n"
     ]
    }
   ],
   "source": [
    "#Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data \n",
    "#and other datasets as train data.\n",
    "\n",
    "#test data\n",
    "bending1_ds1 = dataframes['bending1/dataset1.csv']\n",
    "bending1_ds2 = dataframes['bending1/dataset2.csv']\n",
    "\n",
    "bending2_ds1 = dataframes['bending2/dataset1.csv']\n",
    "bending2_ds2 = dataframes['bending2/dataset2.csv']\n",
    "\n",
    "lying_ds1 = dataframes['lying/dataset1.csv']\n",
    "lying_ds2 = dataframes['lying/dataset2.csv']\n",
    "lying_ds3 = dataframes['lying/dataset3.csv']\n",
    "\n",
    "cycling_ds1 = dataframes['cycling/dataset1.csv']\n",
    "cycling_ds2 = dataframes['cycling/dataset2.csv']\n",
    "cycling_ds3 = dataframes['cycling/dataset3.csv']\n",
    "\n",
    "sitting_ds1 = dataframes['sitting/dataset1.csv']\n",
    "sitting_ds2 = dataframes['sitting/dataset2.csv']\n",
    "sitting_ds3 = dataframes['sitting/dataset3.csv']\n",
    "\n",
    "standing_ds1 = dataframes['standing/dataset1.csv']\n",
    "standing_ds2 = dataframes['standing/dataset2.csv']\n",
    "standing_ds3 = dataframes['standing/dataset3.csv']\n",
    "\n",
    "walking_ds1 = dataframes['walking/dataset1.csv']\n",
    "walking_ds2 = dataframes['walking/dataset2.csv']\n",
    "walking_ds3 = dataframes['walking/dataset3.csv']\n",
    "\n",
    "test_data = pd.concat([\n",
    "    bending1_ds1, bending1_ds2,\n",
    "    bending2_ds1, bending2_ds2,\n",
    "    lying_ds1, lying_ds2, lying_ds3,\n",
    "    cycling_ds1, cycling_ds2, cycling_ds3,\n",
    "    sitting_ds1, sitting_ds2, sitting_ds3,\n",
    "    standing_ds1, standing_ds2, standing_ds3,\n",
    "    walking_ds1, walking_ds2, walking_ds3\n",
    "], ignore_index=True)\n",
    "\n",
    "test_files = ['bending1/dataset1.csv', 'bending1/dataset2.csv',\n",
    "              'bending2/dataset1.csv', 'bending2/dataset2.csv',\n",
    "              'lying/dataset1.csv', 'lying/dataset2.csv', 'lying/dataset3.csv',\n",
    "              'cycling/dataset1.csv', 'cycling/dataset2.csv', 'cycling/dataset3.csv',\n",
    "              'sitting/dataset1.csv', 'sitting/dataset2.csv', 'sitting/dataset3.csv',\n",
    "              'standing/dataset1.csv', 'standing/dataset2.csv', 'standing/dataset3.csv',\n",
    "              'walking/dataset1.csv', 'walking/dataset2.csv', 'walking/dataset3.csv']\n",
    "\n",
    "print(f\"Test Data Shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (33117, 7)\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "training_dataframes = []\n",
    "for file_path, df in dataframes.items():\n",
    "    if file_path not in test_files:\n",
    "        training_dataframes.append(df)\n",
    "\n",
    "training_data = pd.concat(training_dataframes, ignore_index=True)\n",
    "print(f\"Training Data Shape: {training_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research what types of time-domain features are usually used in time series classifcation and list them (examples are minimum, maximum, mean, etc).\n",
    "\n",
    "Time series classification often involves extracting basic statistical features from the data. Commonly used features are the mean, median, standard deviation, minimum, maximum, the range, and interquartile range. There are also distribution features such as skew and tailedness, and even percentiles. For change and trend based features, there are features such as mean absolute deviation, zero crossing rate, mean crossing rate, slope, peak to peak amplitude, number of peaks (local maxima) and number of troughs (local minima). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Instance   min1   max1      mean1  median1      std1   q1_1   q3_1  min2  \\\n",
      "0    lying  23.50  30.00  27.716375     27.5  1.442253  27.00  29.00   0.0   \n",
      "1    lying  24.75  48.33  44.182937     48.0  7.495615  48.00  48.00   0.0   \n",
      "2    lying  48.00  48.25  48.004167     48.0  0.032038  48.00  48.00   0.0   \n",
      "3    lying  34.00  51.00  42.706063     40.5  3.537476  40.25  48.00   0.0   \n",
      "4    lying  39.00  41.00  39.667833     39.5  0.280158  39.50  39.75   0.0   \n",
      "\n",
      "   max2  ...      std5    q1_5     q3_5  min6  max6     mean6  median6  \\\n",
      "0  1.79  ...  4.074511  5.5000  10.7500   0.0  4.50  0.734271     0.71   \n",
      "1  3.11  ...  3.274539  2.0000   5.5425   0.0  3.91  0.692771     0.50   \n",
      "2  0.43  ...  3.268502  4.6700  10.0000   0.0  2.50  0.641229     0.50   \n",
      "3  4.85  ...  4.253807  1.0000   8.0000   0.0  4.97  0.549312     0.47   \n",
      "4  1.00  ...  4.097351  1.6275   9.3300   0.0  3.49  0.635938     0.50   \n",
      "\n",
      "       std6    q1_6  q3_6  \n",
      "0  0.613688  0.4300  1.00  \n",
      "1  0.675781  0.3225  0.94  \n",
      "2  0.388372  0.4600  0.83  \n",
      "3  0.648403  0.0000  0.83  \n",
      "4  0.608399  0.0000  0.83  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "(88, 43)\n"
     ]
    }
   ],
   "source": [
    "#Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series\n",
    "#in each instance. You are free to normalize/standardize features or use them directly\n",
    "\n",
    "#clean data\n",
    "training_data = training_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "\n",
    "#identify feature columns\n",
    "feature_columns = ['avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "\n",
    "def extract_features(instance_df):\n",
    "    #extract time domain features: min, max, mean, median, std, first quartile, third quartile\n",
    "    features = []\n",
    "\n",
    "    for column in feature_columns:\n",
    "        time_series = instance_df[column]\n",
    "        features.extend([time_series.min(), time_series.max(), time_series.mean(), time_series.median(), time_series.std(), time_series.quantile(0.25), time_series.quantile(0.75)])\n",
    "    return features\n",
    "\n",
    "#build feature dataframe\n",
    "feature_rows = []\n",
    "instance_ids = []\n",
    "\n",
    "for file_name, instance_df in dataframes.items():\n",
    "    #extract features for each instance\n",
    "    features = extract_features(instance_df)\n",
    "    feature_rows.append(features)\n",
    "    \n",
    "    #store instance names\n",
    "    instance_id = file_name.split('/')[0]\n",
    "    instance_ids.append(instance_id)\n",
    "\n",
    "#create column names for each feature\n",
    "feature_names = []\n",
    "for i in range(1,7):\n",
    "    feature_names.extend([f'min{i}', f'max{i}', f'mean{i}', f'median{i}', f'std{i}', f'q1_{i}', f'q3_{i}'])\n",
    "    \n",
    "#final dataframe\n",
    "features_df = pd.DataFrame(feature_rows, columns=feature_names)\n",
    "features_df.insert(0, 'Instance', instance_ids)\n",
    "\n",
    "print(features_df.head())\n",
    "print(features_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Standard Deviation  Lower CI   Upper CI\n",
      "0      min1            9.624011  8.314707  10.901471\n",
      "1      max1            4.207745  3.095428   5.111132\n",
      "2     mean1            5.276413  4.628528   5.810407\n",
      "3   median1            5.386624  4.644154   5.969216\n",
      "4      std1            1.771313  1.570297   1.964366\n",
      "5      q1_1            6.128143  5.535231   6.628736\n",
      "6      q3_1            5.031028  4.183576   5.765204\n",
      "7      min2            0.000000  0.000000   0.000000\n",
      "8      max2            5.059656  4.658684   5.381691\n",
      "9     mean2            1.577941  1.389519   1.703054\n",
      "10  median2            1.413593  1.228262   1.542385\n",
      "11     std2            0.885907  0.803776   0.942387\n",
      "12     q1_2            0.948434  0.821274   1.037520\n",
      "13     q3_2            2.131469  1.927499   2.293831\n",
      "14     min3            2.954516  2.754077   3.111398\n",
      "15     max3            4.819848  4.117839   5.425858\n",
      "16    mean3            3.976602  3.336107   4.449722\n",
      "17  median3            4.009687  3.407136   4.512632\n",
      "18     std3            0.951970  0.764319   1.129568\n",
      "19     q1_3            4.184148  3.592681   4.669388\n",
      "20     q3_3            4.153451  3.453892   4.683191\n",
      "21     min4            0.000000  0.000000   0.000000\n",
      "22     max4            2.181809  1.970295   2.368549\n",
      "23    mean4            1.168331  1.078712   1.220597\n",
      "24  median4            1.148992  1.058839   1.205359\n",
      "25     std4            0.458653  0.424323   0.486057\n",
      "26     q1_4            0.844705  0.771604   0.890064\n",
      "27     q3_4            1.555235  1.431685   1.630919\n",
      "28     min5            6.121205  4.463105   7.616450\n",
      "29     max5            5.773355  4.802484   6.599081\n",
      "30    mean5            5.703974  4.415927   6.736266\n",
      "31  median5            5.844368  4.621253   6.891208\n",
      "32     std5            1.012379  0.805726   1.208335\n",
      "33     q1_5            6.122142  4.836718   7.243778\n",
      "34     q3_5            5.563553  4.438877   6.580575\n",
      "35     min6            0.046101  0.000000   0.078476\n",
      "36     max6            2.533515  2.273502   2.759746\n",
      "37    mean6            1.157299  1.066735   1.217011\n",
      "38  median6            1.089858  1.004075   1.154082\n",
      "39     std6            0.517002  0.480329   0.543083\n",
      "40     q1_6            0.761645  0.698478   0.811912\n",
      "41     q3_6            1.527012  1.402120   1.598448\n"
     ]
    }
   ],
   "source": [
    "#Estimate the standard deviation of each of the time-domain features you\n",
    "#extracted from the data. Then, use Python's bootstrapped or any other\n",
    "#method to build a 90% bootsrap confidence interval for the standard deviation\n",
    "#of each feature.\n",
    "\n",
    "std_features = features_df.drop(columns = ['Instance']).std()\n",
    "\n",
    "#convert to dataframe\n",
    "std_features_df = std_features.reset_index()\n",
    "std_features_df.columns = ['Feature', 'Standard Deviation']\n",
    "\n",
    "#bootstrapped confidence intervals\n",
    "#remove columns that are not numeric\n",
    "bootstrap_features = features_df.drop(columns=['Instance'])\n",
    "\n",
    "#bootstrap parameters\n",
    "n_bootstrap = 1000\n",
    "ci_lower = 5\n",
    "ci_upper = 95\n",
    "\n",
    "#store results\n",
    "bootstrap_results = {}\n",
    "\n",
    "for feature in bootstrap_features.columns:\n",
    "    bootstrap_std = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        #sample with replacement\n",
    "        sample = bootstrap_features[feature].sample(n=len(bootstrap_features), replace=True)\n",
    "        #calculate standard deviation\n",
    "        bootstrap_std.append(sample.std())\n",
    "\n",
    "    #calculate confidence interval\n",
    "    lower = np.percentile(bootstrap_std, ci_lower)\n",
    "    upper = np.percentile(bootstrap_std, ci_upper)\n",
    "    bootstrap_results[feature] = (lower, upper)\n",
    "\n",
    "#final std df\n",
    "bootstrap_ci_df = pd.DataFrame.from_dict(bootstrap_results, orient='index', columns=['Lower CI', 'Upper CI'])\n",
    "\n",
    "bootstrap_ci_df = bootstrap_ci_df.reset_index()\n",
    "bootstrap_ci_df.columns = ['Feature', 'Lower CI', 'Upper CI']\n",
    "\n",
    "bootstrap_final = std_features_df.merge(bootstrap_ci_df, on='Feature')\n",
    "\n",
    "print(bootstrap_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Use your judgement to select the three most important time-domain features\n",
    "#(one option may be min, mean, and max).\n",
    "\n",
    "I will select the following three features based on their importance in time series classification: mean, std, and max. The mean will give us a robust measure of central tendancy across all the features, while the standard deviation will give us a sense of variability/spread, important for distinguishing between different activities like walking/standing. Importantly too is the maximum values, which can indicate peaks in the signal, often significant for time series data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.\n",
    "\n",
    "I collect a set of data (n = 100 observations) containing a single\n",
    "predictor and a quantitative response. I then fit a linear regression\n",
    "model to the data, as well as a separate cubic regression, i.e. Y =\n",
    "β0 + β1X + β2X2 + β3X3 + ϵ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ϵ. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "We would expect the cubic regression to have less than or equal training RSS than the linear regression. This is because the cubic model is more flexible/complex, so it can fit the data better than the linear model. Further, it has more parameters to adjust/degrees of freedom, which allows it to fit more of the variance or noise within the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (a) using test rather than training RSS.\n",
    "\n",
    "Visa-versa: the linear regression model would have lower test RSS than the cubic regression. Given that the true relationship between X and Y is linear, such a model would fit the test data best as it is more generalizable with minimal bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "The cubic regression in this case would still have a lower training RSS than the linear regression. Same as above, the cubic regression remains to be the most flexible and can thus fit and adapt to the model better than the linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "Since we don't know the true relationship in this case, it is unclear if linear or a cubc regression would be better. If it is closer to linear, than a linear regression would fit the test data better. However, if it is further from linear, than the cubic regression would fit the test data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###REFERENCES###\n",
    "\n",
    "https://docs.python.org/3/library/zipfile.html\n",
    "\n",
    "https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
    "\n",
    "https://www.geeksforgeeks.org/pandas/convert-bytes-to-a-pandas-dataframe/\n",
    "\n",
    "https://stats.stackexchange.com/questions/50807/features-for-time-series-classification\n",
    "\n",
    "https://medium.com/@rehanmbl/extracting-time-domain-and-frequency-domain-features-from-a-signal-python-implementation-1-2-d36148c949ba\n",
    "\n",
    "https://www.geeksforgeeks.org/data-analysis/feature-engineering-for-time-series-data-methods-and-applications/\n",
    "\n",
    "https://www.kaggle.com/code/oybekeraliev/time-domain-feature-extraction-methods\n",
    "\n",
    "copilot: what does it mean to extract features \"in each instance\"?\n",
    "\n",
    "https://www.askpython.com/python/examples/bootstrap-sampling-introduction\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
